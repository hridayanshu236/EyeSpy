{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport cv2\nimport numpy as np\nimport math\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Reshape, LSTM, Dense,GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nimport albumentations as A\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import random_split, DataLoader\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:07:09.419427Z","iopub.execute_input":"2025-05-28T12:07:09.420161Z","iopub.status.idle":"2025-05-28T12:07:09.425614Z","shell.execute_reply.started":"2025-05-28T12:07:09.420136Z","shell.execute_reply":"2025-05-28T12:07:09.424832Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"image_dir = \"../input/exam-cheating-detection/4000Final/train/images\"\nlabel_dir = \"../input/exam-cheating-detection/4000Final/train/labels\"\n\ndimension_1 = []\ndimension_2 = []\nbase_path = '../input/exam-cheating-detection/4000Final/train/images'\n\n\n# for image_name in os.listdir(base_path):\n#     img_path = os.path.join(base_path, image_name)\n    \n   \n#     img = cv2.imread(img_path)\n    \n#     if img is not None:\n#         # Get the dimensions of the image\n#         d1, d2, d3 = img.shape\n#         dimension_1.append(d1)\n#         dimension_2.append(d2)\n#     else:\n#         print(f\"Error loading image: {img_path}\")\n\n# d1 = np.mean(dimension_1)\n# d2 = np.mean(dimension_2)\n# print(d1,d2)\n\n# cell = cv2.imread(os.path.join(base_path,'10_JPG_jpg.rf.b44bb58adb9dbb4881687aa7b9e17006.jpg'))\n# cell.max()\nimg_shape = (320,320)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:07:09.426698Z","iopub.execute_input":"2025-05-28T12:07:09.427104Z","iopub.status.idle":"2025-05-28T12:07:09.440026Z","shell.execute_reply.started":"2025-05-28T12:07:09.427086Z","shell.execute_reply":"2025-05-28T12:07:09.439506Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"Transform = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n        A.Transpose(p=0.2),\n        A.Normalize(normalization=\"min_max\", p=1.0),\n        A.RandomBrightnessContrast(p=0.3),\n        A.HueSaturationValue(p=0.3),\n        A.RGBShift(p=0.2),\n        A.CLAHE(p=0.1),\n        A.GaussNoise(std_range=(0.1, 0.2), p=0.3), \n        A.ISONoise(p=0.2),\n        A.MotionBlur(blur_limit=5, p=0.2),\n        A.MedianBlur(blur_limit=3, p=0.1),\n        A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n\n        A.RandomScale(scale_limit=0.1, p=0.3),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, border_mode=0, p=0.4),\n        \n        # Adaptive padding and cropping\n        A.PadIfNeeded(min_height=320, min_width=320, border_mode=0),\n        A.RandomCrop(height=320, width=320, p=0.8),\n        A.Resize(height=320, width=320, p=1.0)\n    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:07:09.441012Z","iopub.execute_input":"2025-05-28T12:07:09.441223Z","iopub.status.idle":"2025-05-28T12:07:09.460089Z","shell.execute_reply.started":"2025-05-28T12:07:09.441209Z","shell.execute_reply":"2025-05-28T12:07:09.459419Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"class YOLODataset(Dataset):\n    def __init__(self, images_dir, labels_dir, transform=None):\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n        self.transform = transform\n        self.image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_filename = self.image_files[idx]\n        img_path = os.path.join(self.images_dir, img_filename)\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        height, width, _ = image.shape\n\n        # Load labels\n        label_path = os.path.join(self.labels_dir, os.path.splitext(img_filename)[0] + \".txt\")\n        bboxes, class_labels = [], []\n\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                for line in f:\n                    class_id, x_c, y_c, w, h = map(float, line.strip().split())\n                    x_min = max((x_c - w/2) * width, 0)\n                    y_min = max((y_c - h/2) * height, 0)\n                    x_max = min((x_c + w/2) * width, width - 1)\n                    y_max = min((y_c + h/2) * height, height - 1)\n                    bboxes.append([x_min, y_min, x_max, y_max])\n                    class_labels.append(int(class_id))\n\n        # Apply transforms\n        if self.transform:\n            transformed = self.transform(image=image, bboxes=bboxes, class_labels=class_labels)\n            image = transformed['image']\n            bboxes = transformed['bboxes']\n            class_labels = transformed['class_labels']\n\n        # Normalize and convert to tensor\n        image = image / 255.0\n        image = np.transpose(image, (2, 0, 1))  # HWC â†’ CHW\n        image = torch.tensor(image, dtype=torch.float32)\n\n        # Targets\n        targets = []\n        for box, label in zip(bboxes, class_labels):\n            targets.append([label] + list(box))\n        targets = torch.tensor(targets, dtype=torch.float32)\n\n        return image, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:07:09.461064Z","iopub.execute_input":"2025-05-28T12:07:09.461298Z","iopub.status.idle":"2025-05-28T12:07:09.470081Z","shell.execute_reply.started":"2025-05-28T12:07:09.461278Z","shell.execute_reply":"2025-05-28T12:07:09.469391Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"\ndataset = YOLODataset(\n    images_dir=image_dir,\n    labels_dir=label_dir,\n    transform=Transform\n)\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\ndef collate_fn(batch):\n    images, targets = zip(*batch)\n    images = torch.stack(images)\n    return images, list(targets)\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    collate_fn=collate_fn\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=4,\n    shuffle=False,\n    collate_fn=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:07:09.570092Z","iopub.execute_input":"2025-05-28T12:07:09.570298Z","iopub.status.idle":"2025-05-28T12:07:09.580622Z","shell.execute_reply.started":"2025-05-28T12:07:09.570285Z","shell.execute_reply":"2025-05-28T12:07:09.579987Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"class DetectionLoss(nn.Module):\n    def __init__(self):\n        super(DetectionLoss, self).__init__()\n        self.class_loss_fn = nn.BCEWithLogitsLoss()\n        self.bbox_loss_fn = nn.SmoothL1Loss()\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        predictions: [B, 1, 5]\n        targets: list of [N_i, 5] tensors, N_i can be different for each image\n        We will just use the first target box for each image for now.\n        \"\"\"\n        if predictions.dim() == 3 and predictions.shape[1] == 1:\n            predictions = predictions.squeeze(1)  # [B, 5]\n\n        # Build tensors for labels and boxes by picking the first box for each image\n        batch_size = predictions.shape[0]\n        true_labels = []\n        true_boxes = []\n        for t in targets:\n            if t.shape[0] > 0:\n                true_labels.append(t[0, 0])\n                true_boxes.append(t[0, 1:5])\n            else:\n                # If no targets, treat as background\n                true_labels.append(torch.tensor(0.0, device=predictions.device))\n                true_boxes.append(torch.zeros(4, device=predictions.device))\n        true_labels = torch.stack(true_labels)  # [B]\n        true_boxes = torch.stack(true_boxes)    # [B, 4]\n\n        pred_scores = predictions[:, 0]\n        pred_boxes = predictions[:, 1:5]\n\n        class_loss = self.class_loss_fn(pred_scores, true_labels)\n        bbox_loss = self.bbox_loss_fn(pred_boxes, true_boxes)\n\n        return class_loss + bbox_loss\n\nclass EarlyStopping:\n    def __init__(self, patience=5, restore_best_weights=True):\n        self.patience = patience\n        self.counter = 0\n        self.best_loss = float('inf')\n        self.best_model_state = None\n        self.restore_best_weights = restore_best_weights\n        self.early_stop = False\n\n    def __call__(self, val_loss, model):\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.counter = 0\n            if self.restore_best_weights:\n                self.best_model_state = model.state_dict()\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if self.restore_best_weights:\n                    model.load_state_dict(self.best_model_state)\n\ndef save_checkpoint(model, epoch, val_metric, best_metric, path='best_model.pth'):\n    if val_metric > best_metric:\n        print(f\"Validation metric improved ({best_metric:.4f} â†’ {val_metric:.4f}). Saving model.\")\n        torch.save(model.state_dict(), path)\n        return val_metric\n    return best_metric\n\n\nclass ObjectDetectionCNN(nn.Module):\n    def __init__(self):\n        super(ObjectDetectionCNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.3),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.3),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.4),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.5),\n            nn.Linear(512, 5)  # [objectness_score, x_min, y_min, x_max, y_max]\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x.unsqueeze(1)  # [B, 1, 5]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:07:09.582110Z","iopub.execute_input":"2025-05-28T12:07:09.582355Z","iopub.status.idle":"2025-05-28T12:07:09.596144Z","shell.execute_reply.started":"2025-05-28T12:07:09.582339Z","shell.execute_reply":"2025-05-28T12:07:09.595507Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ObjectDetectionCNN().to(device)\nnum_epochs = 20\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = DetectionLoss()\nearly_stopping = EarlyStopping(patience=5, restore_best_weights=True)\nbest_val_loss = float('inf')\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6, verbose=True)\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n\n    for batch_images, batch_targets in dataloader:\n        batch_images = batch_images.to(device)\n        batch_targets = [t.to(device) for t in batch_targets]  # already correct in your code!\n        outputs = model(batch_images)\n        loss = criterion(outputs, batch_targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    model.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for val_images, val_targets in val_loader:\n            val_images = val_images.to(device)  # Move val images to device INSIDE loop\n            val_targets = [t.to(device) for t in val_targets]\n\n            outputs = model(val_images)\n            loss = criterion(outputs, val_targets)\n            val_loss += loss.item()\n\n    avg_train_loss = train_loss / len(dataloader)\n    avg_val_loss = val_loss / len(val_loader)\n\n    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n\n    # Early stopping\n    early_stopping(avg_val_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping triggered.\")\n        break\n\n    # Reduce LR on plateau\n    scheduler.step(avg_val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T12:07:09.596864Z","iopub.execute_input":"2025-05-28T12:07:09.597079Z","iopub.status.idle":"2025-05-28T13:10:07.138227Z","shell.execute_reply.started":"2025-05-28T12:07:09.597057Z","shell.execute_reply":"2025-05-28T13:10:07.137422Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss = 159.8983, Val Loss = 156.7916\nEpoch 2: Train Loss = 157.6777, Val Loss = 156.2656\nEpoch 3: Train Loss = 153.0074, Val Loss = 149.4646\nEpoch 4: Train Loss = 142.1917, Val Loss = 130.6619\nEpoch 5: Train Loss = 125.8504, Val Loss = 114.9515\nEpoch 6: Train Loss = 104.3935, Val Loss = 92.5925\nEpoch 7: Train Loss = 82.3418, Val Loss = 55.6600\nEpoch 8: Train Loss = 63.9871, Val Loss = 52.7641\nEpoch 9: Train Loss = 54.5098, Val Loss = 50.5295\nEpoch 10: Train Loss = 52.1227, Val Loss = 49.0218\nEpoch 11: Train Loss = 51.5400, Val Loss = 49.3166\nEpoch 12: Train Loss = 51.4064, Val Loss = 49.9826\nEpoch 13: Train Loss = 50.9966, Val Loss = 48.5216\nEpoch 14: Train Loss = 50.9579, Val Loss = 48.9057\nEpoch 15: Train Loss = 50.7145, Val Loss = 47.8009\nEpoch 16: Train Loss = 51.1629, Val Loss = 57.0275\nEpoch 17: Train Loss = 51.0263, Val Loss = 47.5214\nEpoch 18: Train Loss = 50.9372, Val Loss = 49.2975\nEpoch 19: Train Loss = 50.7249, Val Loss = 47.3985\nEpoch 20: Train Loss = 50.6088, Val Loss = 47.7558\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"def simple_accuracy_check(model, test_loader, device, threshold=0.5):\n    \"\"\"Simple accuracy check focusing on objectness prediction\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch_images, batch_targets in test_loader:\n            batch_images = batch_images.to(device)\n            outputs = model(batch_images)\n            \n            for output, target in zip(outputs, batch_targets):\n                # Handle different target formats\n                if isinstance(target, torch.Tensor):\n                    target = target.to(device)\n                    \n                    # Extract objectness from prediction\n                    pred_objectness = torch.sigmoid(output[0, 0]).item()\n                    \n                    # Extract objectness from target - handle different possible formats\n                    if len(target.shape) == 1 and target.shape[0] >= 1:\n                        # If target is [objectness, x_min, y_min, x_max, y_max] format\n                        true_objectness = target[0].item()\n                    elif len(target.shape) == 2:\n                        # If target has extra dimension\n                        true_objectness = target[0, 0].item()\n                    else:\n                        # Fallback - assume first element is objectness\n                        true_objectness = target.flatten()[0].item()\n                        \n                    pred_class = 1 if pred_objectness > threshold else 0\n                    true_class = 1 if true_objectness > 0.5 else 0\n                    \n                    if pred_class == true_class:\n                        correct += 1\n                    total += 1\n                else:\n                    print(f\"Unexpected target type: {type(target)}\")\n                    print(f\"Target content: {target}\")\n                    break\n    \n    if total > 0:\n        accuracy = 100 * correct / total\n        print(f'Simple Objectness Accuracy: {accuracy:.2f}% ({correct}/{total})')\n        return accuracy\n    else:\n        print(\"No samples processed. Check your data format.\")\n        return 0\n\ndef load_model(model_path, device):\n    \"\"\"Load a saved model\"\"\"\n    \n    model = ObjectDetectionCNN().to(device)\n    \n    \n    checkpoint = torch.load(model_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    \n    print(f\"Model loaded from: {model_path}\")\n    print(f\"Training was stopped at epoch: {checkpoint['epoch']}\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:22:36.912459Z","iopub.execute_input":"2025-05-28T13:22:36.912766Z","iopub.status.idle":"2025-05-28T13:22:36.920925Z","shell.execute_reply.started":"2025-05-28T13:22:36.912725Z","shell.execute_reply":"2025-05-28T13:22:36.920306Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"from datetime import datetime\nos.makedirs('saved_models', exist_ok=True)\nmodel_path = f'saved_models/object_detection_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pth'\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'epoch': epoch,\n    'train_loss': avg_train_loss,\n    'val_loss': avg_val_loss,\n    'model_architecture': 'ObjectDetectionCNN'\n}, model_path)\n\nprint(f\"Model saved to: {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:22:38.492489Z","iopub.execute_input":"2025-05-28T13:22:38.492791Z","iopub.status.idle":"2025-05-28T13:22:38.534290Z","shell.execute_reply.started":"2025-05-28T13:22:38.492759Z","shell.execute_reply":"2025-05-28T13:22:38.533570Z"}},"outputs":[{"name":"stdout","text":"Model saved to: saved_models/object_detection_model_20250528_132238.pth\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"test_image_dir = \"../input/exam-cheating-detection/4000Final/test/images\"\ntest_label_dir = \"../input/exam-cheating-detection/4000Final/test/labels\"\ntest_transform = A.Compose([A.Resize(height=320, width=320, p=1.0)])\ntest_dataset = YOLODataset(\n    images_dir=test_image_dir,\n    labels_dir=test_label_dir,\n    transform = test_transform\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=4,\n    shuffle=True,\n    collate_fn=collate_fn\n)\nmodel = load_model('/kaggle/working/saved_models/object_detection_model_20250528_131902.pth',device)\nsimple_accuracy_check(model,test_loader,device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T13:25:15.984662Z","iopub.execute_input":"2025-05-28T13:25:15.984964Z","iopub.status.idle":"2025-05-28T13:25:28.804909Z","shell.execute_reply.started":"2025-05-28T13:25:15.984942Z","shell.execute_reply":"2025-05-28T13:25:28.804142Z"}},"outputs":[{"name":"stdout","text":"Model loaded from: /kaggle/working/saved_models/object_detection_model_20250528_131902.pth\nTraining was stopped at epoch: 19\nSimple Objectness Accuracy: 62.70% (311/496)\n","output_type":"stream"},{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"62.70161290322581"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}