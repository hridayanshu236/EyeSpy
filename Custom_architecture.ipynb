{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-28T12:07:09.420161Z",
     "iopub.status.busy": "2025-05-28T12:07:09.419427Z",
     "iopub.status.idle": "2025-05-28T12:07:09.425614Z",
     "shell.execute_reply": "2025-05-28T12:07:09.424832Z",
     "shell.execute_reply.started": "2025-05-28T12:07:09.420136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Reshape, LSTM, Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T12:07:09.427104Z",
     "iopub.status.busy": "2025-05-28T12:07:09.426698Z",
     "iopub.status.idle": "2025-05-28T12:07:09.440026Z",
     "shell.execute_reply": "2025-05-28T12:07:09.439506Z",
     "shell.execute_reply.started": "2025-05-28T12:07:09.427086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_dir = \"../input/exam-cheating-detection/4000Final/train/images\"\n",
    "label_dir = \"../input/exam-cheating-detection/4000Final/train/labels\"\n",
    "\n",
    "dimension_1 = []\n",
    "dimension_2 = []\n",
    "base_path = '../input/exam-cheating-detection/4000Final/train/images'\n",
    "\n",
    "\n",
    "# for image_name in os.listdir(base_path):\n",
    "#     img_path = os.path.join(base_path, image_name)\n",
    "    \n",
    "   \n",
    "#     img = cv2.imread(img_path)\n",
    "    \n",
    "#     if img is not None:\n",
    "#         # Get the dimensions of the image\n",
    "#         d1, d2, d3 = img.shape\n",
    "#         dimension_1.append(d1)\n",
    "#         dimension_2.append(d2)\n",
    "#     else:\n",
    "#         print(f\"Error loading image: {img_path}\")\n",
    "\n",
    "# d1 = np.mean(dimension_1)\n",
    "# d2 = np.mean(dimension_2)\n",
    "# print(d1,d2)\n",
    "\n",
    "# cell = cv2.imread(os.path.join(base_path,'10_JPG_jpg.rf.b44bb58adb9dbb4881687aa7b9e17006.jpg'))\n",
    "# cell.max()\n",
    "img_shape = (320,320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T12:07:09.441223Z",
     "iopub.status.busy": "2025-05-28T12:07:09.441012Z",
     "iopub.status.idle": "2025-05-28T12:07:09.460089Z",
     "shell.execute_reply": "2025-05-28T12:07:09.459419Z",
     "shell.execute_reply.started": "2025-05-28T12:07:09.441209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Transpose(p=0.2),\n",
    "        A.Normalize(normalization=\"min_max\", p=1.0),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "        A.RGBShift(p=0.2),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.GaussNoise(std_range=(0.1, 0.2), p=0.3), \n",
    "        A.ISONoise(p=0.2),\n",
    "        A.MotionBlur(blur_limit=5, p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "\n",
    "        A.RandomScale(scale_limit=0.1, p=0.3),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, border_mode=0, p=0.4),\n",
    "        \n",
    "        # Adaptive padding and cropping\n",
    "        A.PadIfNeeded(min_height=320, min_width=320, border_mode=0),\n",
    "        A.RandomCrop(height=320, width=320, p=0.8),\n",
    "        A.Resize(height=320, width=320, p=1.0)\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T12:07:09.461298Z",
     "iopub.status.busy": "2025-05-28T12:07:09.461064Z",
     "iopub.status.idle": "2025-05-28T12:07:09.470081Z",
     "shell.execute_reply": "2025-05-28T12:07:09.469391Z",
     "shell.execute_reply.started": "2025-05-28T12:07:09.461278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_filename = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.labels_dir, os.path.splitext(img_filename)[0] + \".txt\")\n",
    "        bboxes, class_labels = [], []\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, x_c, y_c, w, h = map(float, line.strip().split())\n",
    "                    x_min = max((x_c - w/2) * width, 0)\n",
    "                    y_min = max((y_c - h/2) * height, 0)\n",
    "                    x_max = min((x_c + w/2) * width, width - 1)\n",
    "                    y_max = min((y_c + h/2) * height, height - 1)\n",
    "                    bboxes.append([x_min, y_min, x_max, y_max])\n",
    "                    class_labels.append(int(class_id))\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            image = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            class_labels = transformed['class_labels']\n",
    "\n",
    "        # Normalize and convert to tensor\n",
    "        image = image / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1))  # HWC → CHW\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Targets\n",
    "        targets = []\n",
    "        for box, label in zip(bboxes, class_labels):\n",
    "            targets.append([label] + list(box))\n",
    "        targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T12:07:09.570298Z",
     "iopub.status.busy": "2025-05-28T12:07:09.570092Z",
     "iopub.status.idle": "2025-05-28T12:07:09.580622Z",
     "shell.execute_reply": "2025-05-28T12:07:09.579987Z",
     "shell.execute_reply.started": "2025-05-28T12:07:09.570285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = YOLODataset(\n",
    "    images_dir=image_dir,\n",
    "    labels_dir=label_dir,\n",
    "    transform=Transform\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    return images, list(targets)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T12:07:09.582355Z",
     "iopub.status.busy": "2025-05-28T12:07:09.582110Z",
     "iopub.status.idle": "2025-05-28T12:07:09.596144Z",
     "shell.execute_reply": "2025-05-28T12:07:09.595507Z",
     "shell.execute_reply.started": "2025-05-28T12:07:09.582339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DetectionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DetectionLoss, self).__init__()\n",
    "        self.class_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.bbox_loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        predictions: [B, 1, 5]\n",
    "        targets: list of [N_i, 5] tensors, N_i can be different for each image\n",
    "        We will just use the first target box for each image for now.\n",
    "        \"\"\"\n",
    "        if predictions.dim() == 3 and predictions.shape[1] == 1:\n",
    "            predictions = predictions.squeeze(1)  # [B, 5]\n",
    "\n",
    "        # Build tensors for labels and boxes by picking the first box for each image\n",
    "        batch_size = predictions.shape[0]\n",
    "        true_labels = []\n",
    "        true_boxes = []\n",
    "        for t in targets:\n",
    "            if t.shape[0] > 0:\n",
    "                true_labels.append(t[0, 0])\n",
    "                true_boxes.append(t[0, 1:5])\n",
    "            else:\n",
    "                # If no targets, treat as background\n",
    "                true_labels.append(torch.tensor(0.0, device=predictions.device))\n",
    "                true_boxes.append(torch.zeros(4, device=predictions.device))\n",
    "        true_labels = torch.stack(true_labels)  # [B]\n",
    "        true_boxes = torch.stack(true_boxes)    # [B, 4]\n",
    "\n",
    "        pred_scores = predictions[:, 0]\n",
    "        pred_boxes = predictions[:, 1:5]\n",
    "\n",
    "        class_loss = self.class_loss_fn(pred_scores, true_labels)\n",
    "        bbox_loss = self.bbox_loss_fn(pred_boxes, true_boxes)\n",
    "\n",
    "        return class_loss + bbox_loss\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_model_state = model.state_dict()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model_state)\n",
    "\n",
    "def save_checkpoint(model, epoch, val_metric, best_metric, path='best_model.pth'):\n",
    "    if val_metric > best_metric:\n",
    "        print(f\"Validation metric improved ({best_metric:.4f} → {val_metric:.4f}). Saving model.\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        return val_metric\n",
    "    return best_metric\n",
    "\n",
    "\n",
    "class ObjectDetectionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ObjectDetectionCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.4),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 5)  # [objectness_score, x_min, y_min, x_max, y_max]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x.unsqueeze(1)  # [B, 1, 5]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T12:07:09.597079Z",
     "iopub.status.busy": "2025-05-28T12:07:09.596864Z",
     "iopub.status.idle": "2025-05-28T13:10:07.138227Z",
     "shell.execute_reply": "2025-05-28T13:10:07.137422Z",
     "shell.execute_reply.started": "2025-05-28T12:07:09.597057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 159.8983, Val Loss = 156.7916\n",
      "Epoch 2: Train Loss = 157.6777, Val Loss = 156.2656\n",
      "Epoch 3: Train Loss = 153.0074, Val Loss = 149.4646\n",
      "Epoch 4: Train Loss = 142.1917, Val Loss = 130.6619\n",
      "Epoch 5: Train Loss = 125.8504, Val Loss = 114.9515\n",
      "Epoch 6: Train Loss = 104.3935, Val Loss = 92.5925\n",
      "Epoch 7: Train Loss = 82.3418, Val Loss = 55.6600\n",
      "Epoch 8: Train Loss = 63.9871, Val Loss = 52.7641\n",
      "Epoch 9: Train Loss = 54.5098, Val Loss = 50.5295\n",
      "Epoch 10: Train Loss = 52.1227, Val Loss = 49.0218\n",
      "Epoch 11: Train Loss = 51.5400, Val Loss = 49.3166\n",
      "Epoch 12: Train Loss = 51.4064, Val Loss = 49.9826\n",
      "Epoch 13: Train Loss = 50.9966, Val Loss = 48.5216\n",
      "Epoch 14: Train Loss = 50.9579, Val Loss = 48.9057\n",
      "Epoch 15: Train Loss = 50.7145, Val Loss = 47.8009\n",
      "Epoch 16: Train Loss = 51.1629, Val Loss = 57.0275\n",
      "Epoch 17: Train Loss = 51.0263, Val Loss = 47.5214\n",
      "Epoch 18: Train Loss = 50.9372, Val Loss = 49.2975\n",
      "Epoch 19: Train Loss = 50.7249, Val Loss = 47.3985\n",
      "Epoch 20: Train Loss = 50.6088, Val Loss = 47.7558\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ObjectDetectionCNN().to(device)\n",
    "num_epochs = 20\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = DetectionLoss()\n",
    "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "best_val_loss = float('inf')\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6, verbose=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_images, batch_targets in dataloader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_targets = [t.to(device) for t in batch_targets]  # already correct in your code!\n",
    "        outputs = model(batch_images)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_targets in val_loader:\n",
    "            val_images = val_images.to(device)  # Move val images to device INSIDE loop\n",
    "            val_targets = [t.to(device) for t in val_targets]\n",
    "\n",
    "            outputs = model(val_images)\n",
    "            loss = criterion(outputs, val_targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(dataloader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    # Reduce LR on plateau\n",
    "    scheduler.step(avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:22:36.912766Z",
     "iopub.status.busy": "2025-05-28T13:22:36.912459Z",
     "iopub.status.idle": "2025-05-28T13:22:36.920925Z",
     "shell.execute_reply": "2025-05-28T13:22:36.920306Z",
     "shell.execute_reply.started": "2025-05-28T13:22:36.912725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def simple_accuracy_check(model, test_loader, device, threshold=0.5):\n",
    "    \"\"\"Simple accuracy check focusing on objectness prediction\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_targets in test_loader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            outputs = model(batch_images)\n",
    "            \n",
    "            for output, target in zip(outputs, batch_targets):\n",
    "                # Handle different target formats\n",
    "                if isinstance(target, torch.Tensor):\n",
    "                    target = target.to(device)\n",
    "                    \n",
    "                    # Extract objectness from prediction\n",
    "                    pred_objectness = torch.sigmoid(output[0, 0]).item()\n",
    "                    \n",
    "                    # Extract objectness from target - handle different possible formats\n",
    "                    if len(target.shape) == 1 and target.shape[0] >= 1:\n",
    "                        # If target is [objectness, x_min, y_min, x_max, y_max] format\n",
    "                        true_objectness = target[0].item()\n",
    "                    elif len(target.shape) == 2:\n",
    "                        # If target has extra dimension\n",
    "                        true_objectness = target[0, 0].item()\n",
    "                    else:\n",
    "                        # Fallback - assume first element is objectness\n",
    "                        true_objectness = target.flatten()[0].item()\n",
    "                        \n",
    "                    pred_class = 1 if pred_objectness > threshold else 0\n",
    "                    true_class = 1 if true_objectness > 0.5 else 0\n",
    "                    \n",
    "                    if pred_class == true_class:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                else:\n",
    "                    print(f\"Unexpected target type: {type(target)}\")\n",
    "                    print(f\"Target content: {target}\")\n",
    "                    break\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Simple Objectness Accuracy: {accuracy:.2f}% ({correct}/{total})')\n",
    "        return accuracy\n",
    "    else:\n",
    "        print(\"No samples processed. Check your data format.\")\n",
    "        return 0\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    \n",
    "    model = ObjectDetectionCNN().to(device)\n",
    "    \n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded from: {model_path}\")\n",
    "    print(f\"Training was stopped at epoch: {checkpoint['epoch']}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:22:38.492791Z",
     "iopub.status.busy": "2025-05-28T13:22:38.492489Z",
     "iopub.status.idle": "2025-05-28T13:22:38.534290Z",
     "shell.execute_reply": "2025-05-28T13:22:38.533570Z",
     "shell.execute_reply.started": "2025-05-28T13:22:38.492759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_models/object_detection_model_20250528_132238.pth\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "model_path = f'saved_models/object_detection_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'train_loss': avg_train_loss,\n",
    "    'val_loss': avg_val_loss,\n",
    "    'model_architecture': 'ObjectDetectionCNN'\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T13:25:15.984964Z",
     "iopub.status.busy": "2025-05-28T13:25:15.984662Z",
     "iopub.status.idle": "2025-05-28T13:25:28.804909Z",
     "shell.execute_reply": "2025-05-28T13:25:28.804142Z",
     "shell.execute_reply.started": "2025-05-28T13:25:15.984942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: /kaggle/working/saved_models/object_detection_model_20250528_131902.pth\n",
      "Training was stopped at epoch: 19\n",
      "Simple Objectness Accuracy: 62.70% (311/496)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62.70161290322581"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_dir = \"../input/exam-cheating-detection/4000Final/test/images\"\n",
    "test_label_dir = \"../input/exam-cheating-detection/4000Final/test/labels\"\n",
    "test_transform = A.Compose([A.Resize(height=320, width=320, p=1.0)])\n",
    "test_dataset = YOLODataset(\n",
    "    images_dir=test_image_dir,\n",
    "    labels_dir=test_label_dir,\n",
    "    transform = test_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "model = load_model('/kaggle/working/saved_models/object_detection_model_20250528_131902.pth',device)\n",
    "simple_accuracy_check(model,test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
